###########################################################################
## 1.  SVM

from sklearn import svm

X = [[0, 0], [1, 1]]
y = [0, 1]
clf = svm.SVC(gamma='scale')
clf.fit(X, y)
clf.predict([[2., 2.]])


Exemples :

  https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html

  https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html

  https://scikit-learn.org/stable/auto_examples/svm/plot_svm_nonlinear.html

(À noter que le code (py ou ipynb) est téléchargeable en bas de la page.)


###########################################################################
## 2.  Grid search

Afin de trouver les meilleurs hyperparamètres, il existe GridSearchCV().

    from sklearn import svm, datasets
    from sklearn.model_selection import GridSearchCV
    iris = datasets.load_iris()
    parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
    svc = svm.SVC(gamma="scale")
    clf = GridSearchCV(svc, parameters, cv=5)
    clf.fit(iris.data, iris.target)

    sorted(clf.cv_results_.keys())


Et puis, regardez clf.cv_results_

À noter que maintenant il faut faire un train-test-validation split !

Voir aussi  https://scikit-learn.org/stable/modules/grid_search.html .


###########################################################################
## 3.  Évaluation

Plus sur l'évaluation des résultats d'une classification :

    from sklearn.metrics import classification_report, confusion_matrix

    print(confusion_matrix(y_test,y_pred))
    print(classification_report(y_test,y_pred))


Il existe également la validation croisée.

    https://scikit-learn.org/stable/modules/cross_validation.html


###########################################################################
## 4.  Exemple de régression linéaire

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Load the diabetes dataset
diabetes = datasets.load_diabetes()

# Use only one feature
diabetes_X = diabetes.data[:, np.newaxis, 2]

# Split the data into training/testing sets
diabetes_X_train, diabetes_X_test, diabetes_y_train, diabetes_y_test = \
    train_test_split(diabetes_X, diabetes.target, test_size=.3, random_state=42)

# Create linear regression object
regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit(diabetes_X_train, diabetes_y_train)

# Make predictions using the testing set
diabetes_y_pred = regr.predict(diabetes_X_test)

# The coefficients
print('Coefficients: \n', regr.coef_)
# The mean squared error
print("Mean squared error: %.2f"
      % mean_squared_error(diabetes_y_test, diabetes_y_pred))
# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))

# Plot outputs
plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')
plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()


## Exercice : faire la même chose avec validation croisée.


###########################################################################
## 5.  Régression logistique

from sklearn.linear_model import LogisticRegression

x1 = []
for i in range(10):
    for j in range(9 - i):
        x1.append([i, j])

x2 = []
for j in range(10):
    for i in range(10 - j, 10):
        x2.append([i, j])

X = x1 + x2
Y = [1 for a in x1] + [0 for a in x2]

clf = LogisticRegression()
clf.fit(X, Y)
clf.predict([[0, 0]])

h = .02
x_start = y_start = -1
x_end = y_end = 11
xx, yy = np.meshgrid(np.arange(x_start, x_end, h), np.arange(y_start, y_end, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

plt.scatter([a for a,b in x1], [b for a,b in x1], color='r')
plt.scatter([a for a,b in x2], [b for a,b in x2], color='b')
plt.show()
